{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# for developers\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import hdf5plugin # required to access LZ4-encoded HDF5 data sets, if not on your global path\n",
    "import matplotlib.pyplot as plt\n",
    "from diffractem import version, proc2d, pre_proc_opts, io\n",
    "from diffractem.dataset import Dataset\n",
    "from tifffile import imread\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster, TimeoutError\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "opts = pre_proc_opts.PreProcOpts('preproc.yaml')\n",
    "# opts.im_exc = 'indexamajig'\n",
    "cfver = !{opts.im_exc} -v\n",
    "print(f'Running on diffractem:', version())\n",
    "print(f'Running on', cfver[0])\n",
    "print(f'Current path is:', os.getcwd())\n",
    "\n",
    "pxmask=imread(opts.pxmask)\n",
    "reference=imread(opts.reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of SerialED data\n",
    "...from raw diffraction data in _NeXus_ format files to cleaned, sorted, selected, and corrected diffraction data, and accurate information about peak and pattern center positions for further steps. This comprises:\n",
    "* Aggregation of dose fractionation movies\n",
    "* Center and peak finding in diffraction patterns\n",
    "* Hit selection\n",
    "* Export of peak data files for indexing\n",
    "* Flat-field, dead-pixel and saturation correction; optionally background subtraction\n",
    "* Broadcasting of results to single fractionation frames, and different arbitrary cumulations\n",
    "\n",
    "The central tool are `diffractem.Dataset` objects, which handle file I/O, metadata, subset selection, fast computations, and much more.\n",
    "Also, the image processing functions in `diffractem.proc2d` are essential. \n",
    "By internally using the _dask_ framework, Datasets can be larger than memory, computations are lazy (i.e., executed only just when needed) and done in parallel, mostly scaling directly with the number of available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize _dask.distributed_ cluster\n",
    "This initializes the computation backend. If there is no dask.distributed scheduler running at the specified port (usually 8786), a new one will be created. Make sure that the number of workers and threads make sense. It's a good idea to set it to your workstation's CPU configuration, and explicitly set a fast scratch drive as `local_directory`. In the output of the cell you will find the link for the dashboard of the scheduler, where you can follow the computation progress (NB: if you're connecting to Jupyter through a SSH tunnel, you'll need to open an additional tunnel for the port of the dashboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_port = 8786\n",
    "\n",
    "try:\n",
    "    client = Client(address=f'127.0.0.1:{cluster_port}', timeout='2s')\n",
    "    print('Running cluster scheduler found and connected.')\n",
    "    client.run(os.chdir, os.getcwd()); # change the cluster to the current directory\n",
    "except (OSError, TimeoutError):\n",
    "    print('Seems no cluster scheduler is running. Starting one.')\n",
    "    cluster = LocalCluster(host=f'127.0.0.1:{cluster_port}', n_workers=20, threads_per_worker=2, \n",
    "                       local_directory='/scratch/distributed')\n",
    "    client = Client(address=f'127.0.0.1:{cluster_port}')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the raw data set\n",
    "While the files could just be loaded using a wildcard expression in `Dataset.from_files`, it's a good idea to first get an explicit file list using `io.expand_files`, which you can filter in case you find that some files are troublemakers. This list can explicitly be used as input to `Dataset.from_files`.\n",
    "\n",
    "An import aspect now is the _chunking_ of the Dataset, which defines the size of blocks of the dataset that are loaded into memory and processed separately and in parallel. If you did not use dose fractionation, a value around 20 is a good idea. If you did use dose fractionation, it should be larger, and a multiple of the number of frames per crystal. Read more about chunks in the _dask_ documentation.\n",
    "\n",
    "In this data set we had 25 frames per movie, so we pick `chunking=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opts.load() # re-load parameters from the .yaml file\n",
    "\n",
    "raw_files = io.expand_files('raw_data/*.nxs', validate=True)\n",
    "print(f'Found {len(raw_files)} raw files. Have fun pre-processing!')\n",
    "ds = Dataset.from_files(raw_files, chunking=50, )\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge global metadata into shot list\n",
    "Here, we only merge the shutter time of each movie frame (as they varied between the regions in this set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.merge_meta('/%/instrument/detector/collection/shutter_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation/selection\n",
    "Now, shots that correspond to auxiliary scan points have to be rejected (or based on other criteria in the shot list), and, if dose fractionation was used, movies should be summed over some reasonable range of frames before further processing. (In the final steps, we can work on the separate frames again.)\n",
    "\n",
    "Now is a good time to have a first look at the (aggregated) dataset using `tools.viewing_widget`. Note that no data is written to disk yet, all operations are done lazily, i.e. just computed in real time when you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_agg = ds.aggregate(query='frame >= 1 and frame <= 4 and shutter_time == 2', \n",
    "                      by=['sample', 'region', 'run', 'crystal_id'], how='sum', new_folder='proc_data')\n",
    "print(f'Have {len(ds_agg.shots)} shots for processing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a first look\n",
    "Fire up the viewing widget, with some reasonable default parameters.\n",
    "Log is helpful for unprocessed sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "ds_agg.view(Imax=4000, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the peak & center finding pipeline\n",
    "`get_pattern_info` analyzes diffraction patterns (with whatever steps you like - define them in `preproc.yaml`) and returns the results as a pandas DataFrame and a dictionary containing found peak positions in CXI formats. For now, it finds the center of mass (COM), fits a Lorentz function to the central region, finds the peaks, and refines the center position using Friedel mate matching.\n",
    "\n",
    "Here, you can test this pipeline on a small subset of your dataset and check if the peak and center finding work reliably. If not then modify your `preproc.yaml` file parameters, especially those for peak finding under `peak_search_params`.\n",
    "Keep in mind that a too large number of false positive peaks will confuse the indexer horribly.\n",
    "\n",
    "For displaying, consider the `log` checkbox and set `Imax` to something high, so you can properly see if the center is well matched.\n",
    "Then browse a bit through the shots and check the quality of the image annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample = ds_agg.get_random_subset(N=30, seed=1000)\n",
    "ds_sample.compute_pattern_info('preproc.yaml', client=client, output_file=None)\n",
    "ds_sample.view(Imax=4000, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run center & peak finding on full data set\n",
    "Now that you've found the optimal parameters, the same thing is run on the entire data set. We also store a file `image_info.h5`, which is a valid diffractem-type (NeXus) HDF5 file, which contains the metadata and peak/pattern center positions, but no image data. It is useful as a backup or to make export files for indexers. \n",
    "\n",
    "In the second cell, the results are weaved into our data set object. Optionally, you can instead load the data from an existing `image_info.h5` (or similar) file.\n",
    "\n",
    "Note also, that from this point on you can start using `process_peaks.ipynb` to optimize your geometry and unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agg.compute_pattern_info(opts='preproc.yaml', client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agg.merge_pattern_info(ds_from='image_info.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit selection\n",
    "define a criterion to select hits, accordsing to the `selection` query string, then show random sample images to get an idea if it made sense. It often makes sense to not only look at the total `num_peaks`, but also `num_lores_peaks` and `frac_lores_peaks`. The resolution limit for what is considered lores is defined as `lores_limit` in inverse nanometres.\n",
    "\n",
    "Afterwards, you can look (again) at your hit-selected data set `ds_hit` to see if it contains hits only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "selection = 'num_peaks > 15'\n",
    "\n",
    "ds_agg.shots['hit'] = ds_agg.shots.eval(selection)\n",
    "ds_hit = ds_agg.get_selection('hit', file_suffix='_hit.h5')\n",
    "\n",
    "ds_hit.view(Imax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of actual processed images\n",
    "...using the `proc2d.correct_image` function, which applies corrections as defined in the `YAML` file. The stack with corrected images (dask arrays, so they are not actually computed yet!) is added to the dataset.\n",
    "\n",
    "Then, you can use the viewing widget to assess if the correction matches your expectations. If not, change the options file, and re-iterate until you're happy. (NB: The update of the widget might take slightly longer than before, as the correction is done in real time.)\n",
    "\n",
    "Finally, run the `compute_and_save` method, which actually computes all corrected patterns and writes them to disk. Note that using the `exclude_stacks='raw_counts'` will prevent the raw data from being written into the new files. This function might take quite a while. Please follow the progress on the dask dashboard. Note that if you follow the standard workflow, this will be the first time that diffraction data is actually _written_ to disk (besides the virtual one for indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_compute.view(shot=402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the computation. Depending on your computer and data set size, have a coffee or go to bed now.\n",
    "ds_compute.compute_and_save(diff_stack_label='corrected', list_file='hits_agg.lst', exclude_stacks='raw_counts',\n",
    "                            client=client, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Now, you have a set of processed diffraction data files as listed in `hits_agg.lst`, derived from summing (aggregating) over a large range of dose fractionation frames.\n",
    "Those files also contain the found peaks and beam centers which you will require during cell refinement, indexing and integration in `proc_peaks.ipynb` and `indexing.ipynb`; independently those metadata are also written into `image_info.h5`.\n",
    "In `dose_fractionation.ipynb`, it is shown how to prepare similar data files with different frame aggregation ranges, in order to optimize your data for minimum radiation damage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:send]",
   "language": "python",
   "name": "conda-env-send-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
