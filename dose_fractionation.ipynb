{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# for developers\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import hdf5plugin # required to access LZ4-encoded HDF5 data sets, if not on your global path\n",
    "import matplotlib.pyplot as plt\n",
    "from diffractem import version, proc2d, pre_proc_opts, io, tools\n",
    "from diffractem.dataset import Dataset\n",
    "from tifffile import imread\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster, TimeoutError\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "opts = pre_proc_opts.PreProcOpts('preproc.yaml')\n",
    "# opts.im_exc = 'indexamajig'\n",
    "cfver = !{opts.im_exc} -v\n",
    "print(f'Running on diffractem:', version())\n",
    "print(f'Running on', cfver[0])\n",
    "print(f'Current path is:', os.getcwd())\n",
    "\n",
    "pxmask=imread(opts.pxmask)\n",
    "reference=imread(opts.reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of dose-fractionated data sets\n",
    "...which is ideally done after once running the full workflow including indexing, or even merging.\n",
    "Essentially, it starts by creating processed data files, which works very simiarly to `preprocessing.ipynb`.\n",
    "In this script, we will prepare a dataset with aggregated frames 0+1+2, as well as files containing all frames separately, or a cumulative sum of them.\n",
    "\n",
    "Then, `.stream` files with integrated intensities are derived from the already knwon indexing solutions -- similarly to the re-integration as explained in `indexing.ipynb`.\n",
    "\n",
    "After running this notebook, you will have new stream files - one with a different aggregation, one with all single shots, and one with all different aggregations (cumulated single shots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_port = 8786\n",
    "\n",
    "try:\n",
    "    client = Client(address=f'127.0.0.1:{cluster_port}', timeout='2s')\n",
    "    print('Running cluster scheduler found and connected.')\n",
    "    client.run(os.chdir, os.getcwd()); # change the cluster to the current directory\n",
    "except (OSError, TimeoutError):\n",
    "    print('Seems no cluster scheduler is running. Starting one.')\n",
    "    cluster = LocalCluster(host=f'127.0.0.1:{cluster_port}', n_workers=20, threads_per_worker=2, \n",
    "                       local_directory='/scratch/distributed')\n",
    "    client = Client(address=f'127.0.0.1:{cluster_port}')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the raw data set\n",
    "We start, just as in `preprocessing.ipynb`, by loading the raw data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opts.load() # re-load parameters from the .yaml file\n",
    "\n",
    "raw_files = io.expand_files('raw_data/*.nxs', validate=True)\n",
    "print(f'Found {len(raw_files)} raw files. Have fun pre-processing!')\n",
    "ds = Dataset.from_files(raw_files, chunking=50, )\n",
    "ds.merge_meta('/%/instrument/detector/collection/shutter_time')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting to another aggregation\n",
    "\n",
    "We now make an aggregation where we include the first frame (`frame==0`), which we omitted from the original aggregation we used for indexing etc., and only include the first three frames.\n",
    "\n",
    "### Preparing data files\n",
    "...works excatly as for the original aggregation, just that instead of `compute_pattern_info`, we use `merge_pattern_info`, in order to get the pattern information (peaks, center,...) from the `image_info.h5`.\n",
    "\n",
    "From there, it works exactly the same again: do your hit correction, compute the final image using `proc2d.correct_image`, check the outcome using `view`, and compute and save it.\n",
    "After indexing, you can use the newly made files for integration just as well - see `indexing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, e.g. make another aggregation...\n",
    "ds_0to2 = ds.aggregate(query='frame >= 0 and frame <= 2 and shutter_time == 2', \n",
    "                      by=['sample', 'region', 'run', 'crystal_id'], how='sum', \n",
    "                       new_folder='proc_data', file_suffix='_0to2.h5')\n",
    "ds_0to2.merge_pattern_info('image_info.h5')\n",
    "ds_0to2_hit = ds_0to2.get_selection(f'num_peaks > {opts.min_peaks}', file_suffix='_hit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and correct the images\n",
    "opts.load()\n",
    "ds_compute = ds_0to2_hit\n",
    "img_final = proc2d.correct_image(ds_compute.raw_counts, opts,\n",
    "                                ds_compute.shots.lor_x.values,\n",
    "                                ds_compute.shots.lor_y.values,\n",
    "                                ds_compute.peak_data) # keep in mind, that this a lazy computation, so nothing is actually done yet\n",
    "\n",
    "ds_compute.add_stack('corrected', img_final, overwrite=True, set_diff_stack=True)\n",
    "ds_compute.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `ds_0to2_hit` has everything to be written to disk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_0to2_hit.compute_and_save(diff_stack_label='corrected', list_file='hits_0to2.lst', exclude_stacks='raw_counts',\n",
    "                            client=client, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the integration\n",
    "...is virtually identical to the (Re-)Integration step in `indexing.ipynb`.\n",
    "Just make a `.sol` file for your new data set and fire up `indexamajig --indexing=file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dsname = 'hits_0to2'\n",
    "# ds_0to2_hit = Dataset.from_files(dsname + '.lst', open_stacks=False)\n",
    "ds_0to2_hit.get_indexing_solution('master.stream', sol_file=dsname + '.sol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir streams\n",
    "copy_fields = ['sample', 'region', 'crystal_id', 'run', \n",
    "               'adf1', 'adf2', 'lor_hwhm', 'center_x', 'center_y']\n",
    "copy_fields = [f'/%/shots/{cf}' for cf in copy_fields]\n",
    "\n",
    "opts.load()\n",
    "cfcall = tools.call_indexamajig(f'{dsname}.lst', 'refined.geom', \n",
    "                                output=f'streams/{dsname}.stream', \n",
    "                                cell='refined.cell', \n",
    "                                im_params=opts.integration_params, \n",
    "                                procs=40, exc='/opts/crystfel_latest/bin/indexamajig',\n",
    "                                fromfile_input_file = f'{dsname}.sol',\n",
    "                                copy_fields=copy_fields)\n",
    "\n",
    "print('--- RUN THIS ---------------')\n",
    "print(cfcall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting to single shots\n",
    "\n",
    "(Advanced)\n",
    "\n",
    "Now, we want to make a corrected and annotated (i.e., including peaks and centers) version of the raw data, i.e., single movie frames, for example to study radiation damage or be flexible during merging.\n",
    "This is done essentially exactly the same as if you were just using a different aggregation (see above), just that instead of `Dataset.aggregate` you just use `Dataset.get_selection` to restrict the range of included frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, do exactly the same thing as above, but on single-shot data\n",
    "unchunk = False # IMPORTANT: set to True to look at the set with .view(), otherwise set to False\n",
    "\n",
    "ds_sgl = ds.get_selection('frame >= 0 and frame < 10 and shutter_time==2', file_suffix='_allframe.h5', new_folder='proc_data')\n",
    "\n",
    "ds_sgl.merge_pattern_info('image_info.h5')\n",
    "ds_sgl = ds_sgl.get_selection(f'num_peaks > {opts.min_peaks}', file_suffix='_hit.h5')\n",
    "\n",
    "if unchunk:\n",
    "    ds_sgl.rechunk_stacks(1)\n",
    "\n",
    "opts.load()\n",
    "ds_compute = ds_sgl\n",
    "img_final = proc2d.correct_image(ds_compute.raw_counts, opts,\n",
    "                                ds_compute.shots.lor_x.values,\n",
    "                                ds_compute.shots.lor_y.values,\n",
    "                                ds_compute.peak_data) # keep in mind, that this a lazy computation, so nothing is actually done yet\n",
    "\n",
    "ds_compute.add_stack('corrected', img_final, overwrite=True, set_diff_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and run the computation\n",
    "dsname = 'hits-allframe'\n",
    "ds_compute.compute_and_save(diff_stack_label='corrected', list_file=f'{dsname}.lst', exclude_stacks='raw_counts',\n",
    "                            client=client, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally: integrate Bragg spot intensities, grabbing the indexing\n",
    "# solutions from master.stream\n",
    "# IMPORTANT - 'frame' now has to be in the copy_fields, which allows to\n",
    "# later determine which dose fractionation frame a stream chunk belongs to.\n",
    "dsname = 'hits-allframe'\n",
    "ds_sgl.get_indexing_solution('master.stream', sol_file=dsname + '.sol')\n",
    "\n",
    "# IMPORTANT: NOW 'frame' HAS TO BE IN!\n",
    "copy_fields = ['frame','sample', 'region', 'crystal_id', 'run', \n",
    "               'adf1', 'adf2', 'lor_hwhm', 'center_x', 'center_y']\n",
    "copy_fields = [f'/%/shots/{cf}' for cf in copy_fields]\n",
    "\n",
    "opts.load()\n",
    "cfcall = tools.call_indexamajig(f'{dsname}.lst', 'refined.geom', \n",
    "                                output=f'streams/{dsname}.stream', \n",
    "                                cell='refined.cell', \n",
    "                                im_params=opts.integration_params, \n",
    "                                procs=40, exc='/opts/crystfel_latest/bin/indexamajig',\n",
    "                                fromfile_input_file = f'{dsname}.sol',\n",
    "                                copy_fields=copy_fields)\n",
    "\n",
    "print('--- RUN THIS ---------------')\n",
    "print(cfcall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make cumulative-sum files\n",
    "(Advanced)\n",
    "\n",
    "Finally, you can also create a set of files, which instead of single frames, have their cumulative sums, which means that you can pick in hindsight which ones you want to use for the later steps. \n",
    "Some might prefer a workflow where you just make files for different aggregations (as above) that you think make sense.\n",
    "\n",
    "Anyway - for this case, the function `transform_stack_group` does exactly what you want: a cumulative sum over each group in your stack matching one unique crystal. The rest is as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if restarting from here... re-load the single-shot set\n",
    "ds_sgl = Dataset.from_files('hits-allframe.lst', chunking=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data set and apply transform function, which defaults to cumulation\n",
    "ds_cum_0 = ds_sgl.get_selection('True', file_suffix='_cum_from_0.h5')\n",
    "ds_cum_0.transform_stack_groups(stacks='corrected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the computation. Depending on your computer and data set size, have a coffee or go to bed now.\n",
    "dsname = 'hits_cum-0'\n",
    "ds_cum_0.compute_and_save(diff_stack_label='corrected', list_file=f'{dsname}.lst', exclude_stacks='raw_counts',\n",
    "                            client=client, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally: integrate Bragg spot intensities, grabbing the indexing\n",
    "# solutions from master.stream\n",
    "# IMPORTANT - 'frame' now has to be in the copy_fields, which allows to\n",
    "# later determine which dose fractionation frame a stream chunk belongs to.\n",
    "\n",
    "dsname = 'hits_cum-0'\n",
    "ds_cum_0.get_indexing_solution('master.stream', sol_file=dsname + '.sol')\n",
    "\n",
    "# IMPORTANT: NOW 'frame' HAS TO BE IN!\n",
    "copy_fields = ['frame','sample', 'region', 'crystal_id', 'run', \n",
    "               'adf1', 'adf2', 'lor_hwhm', 'center_x', 'center_y']\n",
    "copy_fields = [f'/%/shots/{cf}' for cf in copy_fields]\n",
    "\n",
    "opts.load()\n",
    "cfcall = tools.call_indexamajig(f'{dsname}.lst', 'refined.geom', \n",
    "                                output=f'streams/{dsname}.stream', \n",
    "                                cell='refined.cell', \n",
    "                                im_params=opts.integration_params, \n",
    "                                procs=40, exc='/opts/crystfel_latest/bin/indexamajig',\n",
    "                                fromfile_input_file = f'{dsname}.sol',\n",
    "                                copy_fields=copy_fields)\n",
    "\n",
    "print('--- RUN THIS ---------------')\n",
    "print(cfcall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:send]",
   "language": "python",
   "name": "conda-env-send-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
